\documentclass[12pt,a4paper]{article}

\usepackage[a4paper,text={16.5cm,25.2cm},centering]{geometry}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{mhchem}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.2ex}

\hypersetup
       {   pdfauthor = {  },
           pdftitle={ Introduction to Turing },
           colorlinks=TRUE,
           linkcolor=black,
           citecolor=blue,
           urlcolor=blue
       }

\title{ Introduction to Turing }



\usepackage{upquote}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    basicstyle=\ttfamily\footnotesize,
    upquote=true,
    breaklines=true,
    breakindent=0pt,
    keepspaces=true,
    showspaces=false,
    columns=fullflexible,
    showtabs=false,
    showstringspaces=false,
    escapeinside={(*@}{@*)},
    extendedchars=true,
}
\newcommand{\HLJLt}[1]{#1}
\newcommand{\HLJLw}[1]{#1}
\newcommand{\HLJLe}[1]{#1}
\newcommand{\HLJLeB}[1]{#1}
\newcommand{\HLJLo}[1]{#1}
\newcommand{\HLJLk}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkc}[1]{\textcolor[RGB]{59,151,46}{\textit{#1}}}
\newcommand{\HLJLkd}[1]{\textcolor[RGB]{214,102,97}{\textit{#1}}}
\newcommand{\HLJLkn}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkp}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkr}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkt}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLn}[1]{#1}
\newcommand{\HLJLna}[1]{#1}
\newcommand{\HLJLnb}[1]{#1}
\newcommand{\HLJLnbp}[1]{#1}
\newcommand{\HLJLnc}[1]{#1}
\newcommand{\HLJLncB}[1]{#1}
\newcommand{\HLJLnd}[1]{\textcolor[RGB]{214,102,97}{#1}}
\newcommand{\HLJLne}[1]{#1}
\newcommand{\HLJLneB}[1]{#1}
\newcommand{\HLJLnf}[1]{\textcolor[RGB]{66,102,213}{#1}}
\newcommand{\HLJLnfm}[1]{\textcolor[RGB]{66,102,213}{#1}}
\newcommand{\HLJLnp}[1]{#1}
\newcommand{\HLJLnl}[1]{#1}
\newcommand{\HLJLnn}[1]{#1}
\newcommand{\HLJLno}[1]{#1}
\newcommand{\HLJLnt}[1]{#1}
\newcommand{\HLJLnv}[1]{#1}
\newcommand{\HLJLnvc}[1]{#1}
\newcommand{\HLJLnvg}[1]{#1}
\newcommand{\HLJLnvi}[1]{#1}
\newcommand{\HLJLnvm}[1]{#1}
\newcommand{\HLJLl}[1]{#1}
\newcommand{\HLJLld}[1]{\textcolor[RGB]{148,91,176}{\textit{#1}}}
\newcommand{\HLJLs}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsa}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsb}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsc}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsd}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsdB}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsdC}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLse}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLsh}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsi}[1]{#1}
\newcommand{\HLJLso}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsr}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLss}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLssB}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLnB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnbB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnfB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnh}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLni}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnil}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnoB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLoB}[1]{\textcolor[RGB]{102,102,102}{\textbf{#1}}}
\newcommand{\HLJLow}[1]{\textcolor[RGB]{102,102,102}{\textbf{#1}}}
\newcommand{\HLJLp}[1]{#1}
\newcommand{\HLJLc}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLch}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcm}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcp}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcpB}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcs}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcsB}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLg}[1]{#1}
\newcommand{\HLJLgd}[1]{#1}
\newcommand{\HLJLge}[1]{#1}
\newcommand{\HLJLgeB}[1]{#1}
\newcommand{\HLJLgh}[1]{#1}
\newcommand{\HLJLgi}[1]{#1}
\newcommand{\HLJLgo}[1]{#1}
\newcommand{\HLJLgp}[1]{#1}
\newcommand{\HLJLgs}[1]{#1}
\newcommand{\HLJLgsB}[1]{#1}
\newcommand{\HLJLgt}[1]{#1}


\begin{document}

\maketitle

\subsection{Introduction}
This is the first of a series of tutorials on the universal probabilistic programming language \textbf{Turing}.

Turing is a probabilistic programming system written entirely in Julia. It has an intuitive modelling syntax and supports a wide range of sampling-based inference algorithms. Most importantly, Turing inference is composable: it combines Markov chain sampling operations on subsets of model variables, e.g. using a combination of a Hamiltonian Monte Carlo (HMC) engine and a particle Gibbs (PG) engine. This composable inference engine allows the user to easily switch between black-box style inference methods such as HMC and customized inference methods.

Familiarity with Julia is assumed through out this tutorial. If you are new to Julia, \href{https://julialang.org/learning/}{Learning Julia} is a good starting point.

For users new to Bayesian machine learning, please consider more thorough introductions to the field, such as \href{https://www.springer.com/us/book/9780387310732}{Pattern Recognition and Machine Learning}. This tutorial tries to provide an intuition for Bayesian inference and gives a simple example on how to use Turing. Note that this is not a comprehensive introduction to Bayesian machine learning.

\subsubsection{Coin Flipping Without Turing}
The following example illustrates the effect of updating our beliefs with every piece of new evidence we observe. In particular, assume that we are unsure about the probability of heads in a coin flip. To get an intuitive understanding of what "updating our beliefs" is, we will visualize the probability of heads in a coin flip after each observed evidence.

First, let's load some of the packages we need to flip a coin (\texttt{Random}, \texttt{Distributions}) and show our results (\texttt{Plots}). You will note that Turing is not an import here \ensuremath{\emdash} we do not need it for this example. If you are already familiar with posterior updates, you can proceed to the next step.


\begin{lstlisting}
(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Using}@*) (*@\HLJLcs{Base}@*) (*@\HLJLcs{modules.}@*)
(*@\HLJLk{using}@*) (*@\HLJLn{Random}@*)

(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Load}@*) (*@\HLJLcs{a}@*) (*@\HLJLcs{plotting}@*) (*@\HLJLcs{library.}@*)
(*@\HLJLk{using}@*) (*@\HLJLn{Plots}@*)

(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Load}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{distributions}@*) (*@\HLJLcs{library.}@*)
(*@\HLJLk{using}@*) (*@\HLJLn{Distributions}@*)
\end{lstlisting}


Next, we configure our posterior update model. First, let's set the true probability that any coin flip will turn up heads and set the number of coin flips we will show our model:


\begin{lstlisting}
(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Set}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{true}@*) (*@\HLJLcs{probability}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{heads}@*) (*@\HLJLcs{in}@*) (*@\HLJLcs{a}@*) (*@\HLJLcs{coin.}@*)
(*@\HLJLn{p{\_}true}@*) (*@\HLJLoB{=}@*) (*@\HLJLnfB{0.5}@*)

(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Iterate}@*) (*@\HLJLcs{from}@*) (*@\HLJLcs{having}@*) (*@\HLJLcs{seen}@*) (*@\HLJLcs{0}@*) (*@\HLJLcs{observations}@*) (*@\HLJLcs{to}@*) (*@\HLJLcs{100}@*) (*@\HLJLcs{observations.}@*)
(*@\HLJLn{Ns}@*) (*@\HLJLoB{=}@*) (*@\HLJLni{0}@*)(*@\HLJLoB{:}@*)(*@\HLJLni{100}@*)(*@\HLJLp{;}@*)
\end{lstlisting}


We will now use the Bernoulli distribution to flip 100 coins, and collect the results in a variable called \texttt{data}:


\begin{lstlisting}
(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Draw}@*) (*@\HLJLcs{data}@*) (*@\HLJLcs{from}@*) (*@\HLJLcs{a}@*) (*@\HLJLcs{Bernoulli}@*) (*@\HLJLcs{distribution,}@*) (*@\HLJLcs{i.e.}@*) (*@\HLJLcs{draw}@*) (*@\HLJLcs{heads}@*) (*@\HLJLcs{or}@*) (*@\HLJLcs{tails.}@*)
(*@\HLJLn{Random}@*)(*@\HLJLoB{.}@*)(*@\HLJLnf{seed!}@*)(*@\HLJLp{(}@*)(*@\HLJLni{12}@*)(*@\HLJLp{)}@*)
(*@\HLJLn{data}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{rand}@*)(*@\HLJLp{(}@*)(*@\HLJLnf{Bernoulli}@*)(*@\HLJLp{(}@*)(*@\HLJLn{p{\_}true}@*)(*@\HLJLp{),}@*) (*@\HLJLnf{last}@*)(*@\HLJLp{(}@*)(*@\HLJLn{Ns}@*)(*@\HLJLp{))}@*)

(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Here{\textquotesingle}s}@*) (*@\HLJLcs{what}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{first}@*) (*@\HLJLcs{five}@*) (*@\HLJLcs{coin}@*) (*@\HLJLcs{flips}@*) (*@\HLJLcs{look}@*) (*@\HLJLcs{like:}@*)
(*@\HLJLn{data}@*)(*@\HLJLp{[}@*)(*@\HLJLni{1}@*)(*@\HLJLoB{:}@*)(*@\HLJLni{5}@*)(*@\HLJLp{]}@*)
\end{lstlisting}

\begin{lstlisting}
5-element Vector(*@{{\{}}@*)Bool(*@{{\}}}@*):
 1
 0
 1
 1
 0
\end{lstlisting}


After flipping all our coins, we want to set a prior belief about what we think the distribution of coin flips look like. In this case, we are going to choose a common prior distribution called the \href{https://en.wikipedia.org/wiki/Beta_distribution}{Beta} distribution.


\begin{lstlisting}
(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Our}@*) (*@\HLJLcs{prior}@*) (*@\HLJLcs{belief}@*) (*@\HLJLcs{about}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{probability}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{heads}@*) (*@\HLJLcs{in}@*) (*@\HLJLcs{a}@*) (*@\HLJLcs{coin}@*) (*@\HLJLcs{toss.}@*)
(*@\HLJLn{prior{\_}belief}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{Beta}@*)(*@\HLJLp{(}@*)(*@\HLJLni{1}@*)(*@\HLJLp{,}@*) (*@\HLJLni{1}@*)(*@\HLJLp{);}@*)
\end{lstlisting}


With our priors set and our data at hand, we can perform Bayesian inference.

This is a fairly simple process. We expose one additional coin flip to our model every iteration, such that the first run only sees the first coin flip, while the last iteration sees all the coin flips. Then, we set the \texttt{updated\_belief} variable to an updated version of the original Beta distribution that accounts for the new proportion of heads and tails. 

For the mathematically inclined, the \texttt{Beta} distribution is updated by adding each coin flip to the distribution's $\alpha$ and $\beta$ parameters, which are initially defined as $\alpha = 1, \beta = 1$. Over time, with more and more coin flips, $\alpha$ and $\beta$ will be approximately equal to each other as we are equally likely to flip a heads or a tails, and the plot of the beta distribution will become more tightly centered around 0.5. 

This works because mean of the \texttt{Beta} distribution is defined as the following:

\[
\text{E}[\text{Beta}] = \dfrac{\alpha}{\alpha+\beta}
\]
Which is 0.5 when $\alpha = \beta$, as we expect for a large enough number of coin flips. As we increase the number of samples, our variance will also decrease, such that the distribution will reflect less uncertainty about the probability of receiving a heads. The definition of the variance for the \texttt{Beta} distribution is the following:

\[
\text{var}[\text{Beta}] = \dfrac{\alpha\beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
\]
The intuition about this definition is that the variance of the distribution will approach 0 with more and more samples, as the denominator will grow faster than will the numerator. More samples means less variance.


\begin{lstlisting}
(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Import}@*) (*@\HLJLcs{StatsPlots}@*) (*@\HLJLcs{for}@*) (*@\HLJLcs{animating}@*) (*@\HLJLcs{purposes.}@*)
(*@\HLJLk{using}@*) (*@\HLJLn{StatsPlots}@*)

(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Make}@*) (*@\HLJLcs{an}@*) (*@\HLJLcs{animation.}@*)
(*@\HLJLn{animation}@*) (*@\HLJLoB{=}@*) (*@\HLJLnd{@gif}@*) (*@\HLJLk{for}@*) (*@\HLJLp{(}@*)(*@\HLJLn{i}@*)(*@\HLJLp{,}@*) (*@\HLJLn{N}@*)(*@\HLJLp{)}@*) (*@\HLJLkp{in}@*) (*@\HLJLnf{enumerate}@*)(*@\HLJLp{(}@*)(*@\HLJLn{Ns}@*)(*@\HLJLp{)}@*)

    (*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Count}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{number}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{heads}@*) (*@\HLJLcs{and}@*) (*@\HLJLcs{tails.}@*)
    (*@\HLJLn{heads}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{sum}@*)(*@\HLJLp{(}@*)(*@\HLJLn{data}@*)(*@\HLJLp{[}@*)(*@\HLJLni{1}@*)(*@\HLJLoB{:}@*)(*@\HLJLn{i}@*)(*@\HLJLoB{-}@*)(*@\HLJLni{1}@*)(*@\HLJLp{])}@*)
    (*@\HLJLn{tails}@*) (*@\HLJLoB{=}@*) (*@\HLJLn{N}@*) (*@\HLJLoB{-}@*) (*@\HLJLn{heads}@*)
    
    (*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Update}@*) (*@\HLJLcs{our}@*) (*@\HLJLcs{prior}@*) (*@\HLJLcs{belief}@*) (*@\HLJLcs{in}@*) (*@\HLJLcs{closed}@*) (*@\HLJLcs{form}@*) (*@\HLJLcs{(this}@*) (*@\HLJLcs{is}@*) (*@\HLJLcs{possible}@*) (*@\HLJLcs{because}@*) (*@\HLJLcs{we}@*) (*@\HLJLcs{use}@*) (*@\HLJLcs{a}@*) (*@\HLJLcs{conjugate}@*) (*@\HLJLcs{prior).}@*)
    (*@\HLJLn{updated{\_}belief}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{Beta}@*)(*@\HLJLp{(}@*)(*@\HLJLn{prior{\_}belief}@*)(*@\HLJLoB{.}@*)(*@\HLJLn{\ensuremath{\alpha}}@*) (*@\HLJLoB{+}@*) (*@\HLJLn{heads}@*)(*@\HLJLp{,}@*) (*@\HLJLn{prior{\_}belief}@*)(*@\HLJLoB{.}@*)(*@\HLJLn{\ensuremath{\beta}}@*) (*@\HLJLoB{+}@*) (*@\HLJLn{tails}@*)(*@\HLJLp{)}@*)

    (*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Plotting}@*)
    (*@\HLJLnf{plot}@*)(*@\HLJLp{(}@*)(*@\HLJLn{updated{\_}belief}@*)(*@\HLJLp{,}@*) 
        (*@\HLJLn{size}@*) (*@\HLJLoB{=}@*) (*@\HLJLp{(}@*)(*@\HLJLni{500}@*)(*@\HLJLp{,}@*) (*@\HLJLni{250}@*)(*@\HLJLp{),}@*) 
        (*@\HLJLn{title}@*) (*@\HLJLoB{=}@*) (*@\HLJLs{"{}Updated}@*) (*@\HLJLs{belief}@*) (*@\HLJLs{after}@*) (*@\HLJLsi{{\$}N}@*) (*@\HLJLs{observations"{}}@*)(*@\HLJLp{,}@*)
        (*@\HLJLn{xlabel}@*) (*@\HLJLoB{=}@*) (*@\HLJLs{"{}probability}@*) (*@\HLJLs{of}@*) (*@\HLJLs{heads"{}}@*)(*@\HLJLp{,}@*) 
        (*@\HLJLn{ylabel}@*) (*@\HLJLoB{=}@*) (*@\HLJLs{"{}"{}}@*)(*@\HLJLp{,}@*) 
        (*@\HLJLn{legend}@*) (*@\HLJLoB{=}@*) (*@\HLJLn{nothing}@*)(*@\HLJLp{,}@*)
        (*@\HLJLn{xlim}@*) (*@\HLJLoB{=}@*) (*@\HLJLp{(}@*)(*@\HLJLni{0}@*)(*@\HLJLp{,}@*)(*@\HLJLni{1}@*)(*@\HLJLp{),}@*)
        (*@\HLJLn{fill}@*)(*@\HLJLoB{=}@*)(*@\HLJLni{0}@*)(*@\HLJLp{,}@*) (*@\HLJLn{\ensuremath{\alpha}}@*)(*@\HLJLoB{=}@*)(*@\HLJLnfB{0.3}@*)(*@\HLJLp{,}@*) (*@\HLJLn{w}@*)(*@\HLJLoB{=}@*)(*@\HLJLni{3}@*)(*@\HLJLp{)}@*)
    (*@\HLJLnf{vline!}@*)(*@\HLJLp{([}@*)(*@\HLJLn{p{\_}true}@*)(*@\HLJLp{])}@*)
(*@\HLJLk{end}@*)
\end{lstlisting}

\includegraphics[width=\linewidth]{jl_USrzxA/00_introduction_5_1.gif}

The animation above shows that with increasing evidence our belief about the probability of heads in a coin flip slowly adjusts towards the true value. The orange line in the animation represents the true probability of seeing heads on a single coin flip, while the mode of the distribution shows what the model believes the probability of a heads is given the evidence it has seen.

\subsubsection{Coin Flipping With Turing}
In the previous example, we used the fact that our prior distribution is a \href{https://en.wikipedia.org/wiki/Conjugate_prior}{conjugate prior}. Note that a closed-form expression (the \texttt{updated\_belief} expression) for the posterior is not accessible in general and usually does not exist for more interesting models. 

We are now going to move away from the closed-form expression above and specify the same model using \textbf{Turing}. To do so, we will first need to import \texttt{Turing}, \texttt{MCMCChains}, \texttt{Distributions}, and \texttt{StatPlots}. \texttt{MCMCChains} is a library built by the Turing team to help summarize Markov Chain Monte Carlo (MCMC) simulations, as well as a variety of utility functions for diagnostics and visualizations.


\begin{lstlisting}
(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Load}@*) (*@\HLJLcs{Turing}@*) (*@\HLJLcs{and}@*) (*@\HLJLcs{MCMCChains.}@*)
(*@\HLJLk{using}@*) (*@\HLJLn{Turing}@*)(*@\HLJLp{,}@*) (*@\HLJLn{MCMCChains}@*)

(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Load}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{distributions}@*) (*@\HLJLcs{library.}@*)
(*@\HLJLk{using}@*) (*@\HLJLn{Distributions}@*)

(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Load}@*) (*@\HLJLcs{StatsPlots}@*) (*@\HLJLcs{for}@*) (*@\HLJLcs{density}@*) (*@\HLJLcs{plots.}@*)
(*@\HLJLk{using}@*) (*@\HLJLn{StatsPlots}@*)
\end{lstlisting}


First, we define the coin-flip model using Turing.


\begin{lstlisting}
(*@\HLJLnd{@model}@*) (*@\HLJLnf{coinflip}@*)(*@\HLJLp{(}@*)(*@\HLJLn{y}@*)(*@\HLJLp{)}@*) (*@\HLJLoB{=}@*) (*@\HLJLk{begin}@*)
    
    (*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Our}@*) (*@\HLJLcs{prior}@*) (*@\HLJLcs{belief}@*) (*@\HLJLcs{about}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{probability}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{heads}@*) (*@\HLJLcs{in}@*) (*@\HLJLcs{a}@*) (*@\HLJLcs{coin.}@*)
    (*@\HLJLn{p}@*) (*@\HLJLoB{{\textasciitilde}}@*) (*@\HLJLnf{Beta}@*)(*@\HLJLp{(}@*)(*@\HLJLni{1}@*)(*@\HLJLp{,}@*) (*@\HLJLni{1}@*)(*@\HLJLp{)}@*)
    
    (*@\HLJLcs{{\#}}@*) (*@\HLJLcs{The}@*) (*@\HLJLcs{number}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{observations.}@*)
    (*@\HLJLn{N}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{length}@*)(*@\HLJLp{(}@*)(*@\HLJLn{y}@*)(*@\HLJLp{)}@*)
    (*@\HLJLk{for}@*) (*@\HLJLn{n}@*) (*@\HLJLkp{in}@*) (*@\HLJLni{1}@*)(*@\HLJLoB{:}@*)(*@\HLJLn{N}@*)
        (*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Heads}@*) (*@\HLJLcs{or}@*) (*@\HLJLcs{tails}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{a}@*) (*@\HLJLcs{coin}@*) (*@\HLJLcs{are}@*) (*@\HLJLcs{drawn}@*) (*@\HLJLcs{from}@*) (*@\HLJLcs{a}@*) (*@\HLJLcs{Bernoulli}@*) (*@\HLJLcs{distribution.}@*)
        (*@\HLJLn{y}@*)(*@\HLJLp{[}@*)(*@\HLJLn{n}@*)(*@\HLJLp{]}@*) (*@\HLJLoB{{\textasciitilde}}@*) (*@\HLJLnf{Bernoulli}@*)(*@\HLJLp{(}@*)(*@\HLJLn{p}@*)(*@\HLJLp{)}@*)
    (*@\HLJLk{end}@*)
(*@\HLJLk{end}@*)(*@\HLJLp{;}@*)
\end{lstlisting}


After defining the model, we can approximate the posterior distribution by drawing samples from the distribution. In this example, we use a \href{https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo}{Hamiltonian Monte Carlo} sampler to draw these samples. Later tutorials will give more information on the samplers available in Turing and discuss their use for different models.


\begin{lstlisting}
(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Settings}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{Hamiltonian}@*) (*@\HLJLcs{Monte}@*) (*@\HLJLcs{Carlo}@*) (*@\HLJLcs{(HMC)}@*) (*@\HLJLcs{sampler.}@*)
(*@\HLJLn{iterations}@*) (*@\HLJLoB{=}@*) (*@\HLJLni{1000}@*)
(*@\HLJLn{\ensuremath{\epsilon}}@*) (*@\HLJLoB{=}@*) (*@\HLJLnfB{0.05}@*)
(*@\HLJLn{\ensuremath{\tau}}@*) (*@\HLJLoB{=}@*) (*@\HLJLni{10}@*)

(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Start}@*) (*@\HLJLcs{sampling.}@*)
(*@\HLJLn{chain}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{sample}@*)(*@\HLJLp{(}@*)(*@\HLJLnf{coinflip}@*)(*@\HLJLp{(}@*)(*@\HLJLn{data}@*)(*@\HLJLp{),}@*) (*@\HLJLnf{HMC}@*)(*@\HLJLp{(}@*)(*@\HLJLn{\ensuremath{\epsilon}}@*)(*@\HLJLp{,}@*) (*@\HLJLn{\ensuremath{\tau}}@*)(*@\HLJLp{),}@*) (*@\HLJLn{iterations}@*)(*@\HLJLp{,}@*) (*@\HLJLn{progress}@*)(*@\HLJLoB{=}@*)(*@\HLJLkc{false}@*)(*@\HLJLp{);}@*)
\end{lstlisting}


After finishing the sampling process, we can visualize the posterior distribution approximated using Turing against the posterior distribution in closed-form. We can extract the chain data from the sampler using the \texttt{Chains(chain[:p])} function, exported from the \texttt{MCMCChain} module. \texttt{Chains(chain[:p])} creates an instance of the \texttt{Chain} type which summarizes the MCMC simulation \ensuremath{\emdash} the \texttt{MCMCChain} module supports numerous tools for plotting, summarizing, and describing variables of type \texttt{Chain}.


\begin{lstlisting}
(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Construct}@*) (*@\HLJLcs{summary}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{sampling}@*) (*@\HLJLcs{process}@*) (*@\HLJLcs{for}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{parameter}@*) (*@\HLJLcs{p,}@*) (*@\HLJLcs{i.e.}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{probability}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{heads}@*) (*@\HLJLcs{in}@*) (*@\HLJLcs{a}@*) (*@\HLJLcs{coin.}@*)
(*@\HLJLn{p{\_}summary}@*) (*@\HLJLoB{=}@*) (*@\HLJLn{chain}@*)(*@\HLJLp{[}@*)(*@\HLJLsc{:p}@*)(*@\HLJLp{]}@*)
(*@\HLJLnf{plot}@*)(*@\HLJLp{(}@*)(*@\HLJLn{p{\_}summary}@*)(*@\HLJLp{,}@*) (*@\HLJLn{seriestype}@*) (*@\HLJLoB{=}@*) (*@\HLJLsc{:histogram}@*)(*@\HLJLp{)}@*)
\end{lstlisting}

\includegraphics[width=\linewidth]{jl_USrzxA/00_introduction_9_1.pdf}

Now we can build our plot:


\begin{lstlisting}
(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Compute}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{posterior}@*) (*@\HLJLcs{distribution}@*) (*@\HLJLcs{in}@*) (*@\HLJLcs{closed-form.}@*)
(*@\HLJLn{N}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{length}@*)(*@\HLJLp{(}@*)(*@\HLJLn{data}@*)(*@\HLJLp{)}@*)
(*@\HLJLn{heads}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{sum}@*)(*@\HLJLp{(}@*)(*@\HLJLn{data}@*)(*@\HLJLp{)}@*)
(*@\HLJLn{updated{\_}belief}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{Beta}@*)(*@\HLJLp{(}@*)(*@\HLJLn{prior{\_}belief}@*)(*@\HLJLoB{.}@*)(*@\HLJLn{\ensuremath{\alpha}}@*) (*@\HLJLoB{+}@*) (*@\HLJLn{heads}@*)(*@\HLJLp{,}@*) (*@\HLJLn{prior{\_}belief}@*)(*@\HLJLoB{.}@*)(*@\HLJLn{\ensuremath{\beta}}@*) (*@\HLJLoB{+}@*) (*@\HLJLn{N}@*) (*@\HLJLoB{-}@*) (*@\HLJLn{heads}@*)(*@\HLJLp{)}@*)

(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Visualize}@*) (*@\HLJLcs{a}@*) (*@\HLJLcs{blue}@*) (*@\HLJLcs{density}@*) (*@\HLJLcs{plot}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{approximate}@*) (*@\HLJLcs{posterior}@*) (*@\HLJLcs{distribution}@*) (*@\HLJLcs{using}@*) (*@\HLJLcs{HMC}@*) (*@\HLJLcs{(see}@*) (*@\HLJLcs{Chain}@*) (*@\HLJLcs{1}@*) (*@\HLJLcs{in}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{legend).}@*)
(*@\HLJLn{p}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{plot}@*)(*@\HLJLp{(}@*)(*@\HLJLn{p{\_}summary}@*)(*@\HLJLp{,}@*) (*@\HLJLn{seriestype}@*) (*@\HLJLoB{=}@*) (*@\HLJLsc{:density}@*)(*@\HLJLp{,}@*) (*@\HLJLn{xlim}@*) (*@\HLJLoB{=}@*) (*@\HLJLp{(}@*)(*@\HLJLni{0}@*)(*@\HLJLp{,}@*)(*@\HLJLni{1}@*)(*@\HLJLp{),}@*) (*@\HLJLn{legend}@*) (*@\HLJLoB{=}@*) (*@\HLJLsc{:best}@*)(*@\HLJLp{,}@*) (*@\HLJLn{w}@*) (*@\HLJLoB{=}@*) (*@\HLJLni{2}@*)(*@\HLJLp{,}@*) (*@\HLJLn{c}@*) (*@\HLJLoB{=}@*) (*@\HLJLsc{:blue}@*)(*@\HLJLp{)}@*)

(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Visualize}@*) (*@\HLJLcs{a}@*) (*@\HLJLcs{green}@*) (*@\HLJLcs{density}@*) (*@\HLJLcs{plot}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{posterior}@*) (*@\HLJLcs{distribution}@*) (*@\HLJLcs{in}@*) (*@\HLJLcs{closed-form.}@*)
(*@\HLJLnf{plot!}@*)(*@\HLJLp{(}@*)(*@\HLJLn{p}@*)(*@\HLJLp{,}@*) (*@\HLJLnf{range}@*)(*@\HLJLp{(}@*)(*@\HLJLni{0}@*)(*@\HLJLp{,}@*) (*@\HLJLn{stop}@*) (*@\HLJLoB{=}@*) (*@\HLJLni{1}@*)(*@\HLJLp{,}@*) (*@\HLJLn{length}@*) (*@\HLJLoB{=}@*) (*@\HLJLni{100}@*)(*@\HLJLp{),}@*) (*@\HLJLn{pdf}@*)(*@\HLJLoB{.}@*)(*@\HLJLp{(}@*)(*@\HLJLnf{Ref}@*)(*@\HLJLp{(}@*)(*@\HLJLn{updated{\_}belief}@*)(*@\HLJLp{),}@*) (*@\HLJLnf{range}@*)(*@\HLJLp{(}@*)(*@\HLJLni{0}@*)(*@\HLJLp{,}@*) (*@\HLJLn{stop}@*) (*@\HLJLoB{=}@*) (*@\HLJLni{1}@*)(*@\HLJLp{,}@*) (*@\HLJLn{length}@*) (*@\HLJLoB{=}@*) (*@\HLJLni{100}@*)(*@\HLJLp{)),}@*) 
        (*@\HLJLn{xlabel}@*) (*@\HLJLoB{=}@*) (*@\HLJLs{"{}probability}@*) (*@\HLJLs{of}@*) (*@\HLJLs{heads"{}}@*)(*@\HLJLp{,}@*) (*@\HLJLn{ylabel}@*) (*@\HLJLoB{=}@*) (*@\HLJLs{"{}"{}}@*)(*@\HLJLp{,}@*) (*@\HLJLn{title}@*) (*@\HLJLoB{=}@*) (*@\HLJLs{"{}"{}}@*)(*@\HLJLp{,}@*) (*@\HLJLn{xlim}@*) (*@\HLJLoB{=}@*) (*@\HLJLp{(}@*)(*@\HLJLni{0}@*)(*@\HLJLp{,}@*)(*@\HLJLni{1}@*)(*@\HLJLp{),}@*) (*@\HLJLn{label}@*) (*@\HLJLoB{=}@*) (*@\HLJLs{"{}Closed-form"{}}@*)(*@\HLJLp{,}@*)
        (*@\HLJLn{fill}@*)(*@\HLJLoB{=}@*)(*@\HLJLni{0}@*)(*@\HLJLp{,}@*) (*@\HLJLn{\ensuremath{\alpha}}@*)(*@\HLJLoB{=}@*)(*@\HLJLnfB{0.3}@*)(*@\HLJLp{,}@*) (*@\HLJLn{w}@*)(*@\HLJLoB{=}@*)(*@\HLJLni{3}@*)(*@\HLJLp{,}@*) (*@\HLJLn{c}@*) (*@\HLJLoB{=}@*) (*@\HLJLsc{:lightgreen}@*)(*@\HLJLp{)}@*)

(*@\HLJLcs{{\#}}@*) (*@\HLJLcs{Visualize}@*) (*@\HLJLcs{the}@*) (*@\HLJLcs{true}@*) (*@\HLJLcs{probability}@*) (*@\HLJLcs{of}@*) (*@\HLJLcs{heads}@*) (*@\HLJLcs{in}@*) (*@\HLJLcs{red.}@*)
(*@\HLJLnf{vline!}@*)(*@\HLJLp{(}@*)(*@\HLJLn{p}@*)(*@\HLJLp{,}@*) (*@\HLJLp{[}@*)(*@\HLJLn{p{\_}true}@*)(*@\HLJLp{],}@*) (*@\HLJLn{label}@*) (*@\HLJLoB{=}@*) (*@\HLJLs{"{}True}@*) (*@\HLJLs{probability"{}}@*)(*@\HLJLp{,}@*) (*@\HLJLn{c}@*) (*@\HLJLoB{=}@*) (*@\HLJLsc{:red}@*)(*@\HLJLp{)}@*)
\end{lstlisting}

\includegraphics[width=\linewidth]{jl_USrzxA/00_introduction_10_1.pdf}

As we can see, the Turing model closely approximates the true probability. Hopefully this tutorial has provided an easy-to-follow, yet informative introduction to Turing's simpler applications. More advanced usage will be demonstrated in later tutorials.



\subsection{Appendix}
This tutorial is part of the TuringTutorials repository, found at: \href{https://github.com/TuringLang/TuringTutorials}{https://github.com/TuringLang/TuringTutorials}.


To locally run this tutorial, do the following commands:

\begin{verbatim}
using TuringTutorials
TuringTutorials.weave_file("00-introduction/", "00_introduction.jmd")
\end{verbatim}

Computer Information:


\begin{verbatim}
Julia Version 1.6.0
Commit f9720dc2eb (2021-03-24 12:55 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-11.0.1 (ORCJIT, skylake)
Environment:
  JULIA_CMDSTAN_HOME = /home/cameron/stan/
  JULIA_NUM_THREADS = 16

\end{verbatim}

Package Information:


\begin{verbatim}
      Status `~/.julia/dev/TuringTutorials/tutorials/00-introduction/Project.toml`
  [31c24e10] Distributions v0.24.15
  [c7f686f2] MCMCChains v4.7.3
  [91a5bcdd] Plots v1.11.1
  [f3b207a7] StatsPlots v0.14.19
  [fce5fe82] Turing v0.15.12
  [9a3f8284] Random

\end{verbatim}



\end{document}
